{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "0d008810b9c8467bcb3ca39aa2180e5b81b3a9acb136aab30d47954377cc5120"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.core.fromnumeric import argmin\n",
    "import handin1 as func\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "def is_pos_def(x):\n",
    "    return np.all(np.linalg.eigvals(x) > 0)\n",
    "\n",
    "def trust_region_subproblem(gradient, hessian, lambda_0, delta, max_iter=1000):\n",
    "    d = hessian.shape[0]\n",
    "    lambda_j, vector = np.linalg.eig(hessian)\n",
    "    lambda_1 = min(lambda_j)\n",
    "    min_index = argmin(lambda_j)\n",
    "    lam = lambda_0\n",
    "    p = 0\n",
    "    count = 0\n",
    "    while lam >= -lambda_1:\n",
    "        count += 1\n",
    "        B_ = hessian + lam * np.identity(d)\n",
    "        # try:\n",
    "        if vector[:, min_index].T @ gradient == 0:\n",
    "            p = hard_case(gradient, hessian, delta, lambda_j, vector, min_index)\n",
    "            return p\n",
    "        else:\n",
    "            p = np.linalg.inv(B_) @ (-gradient)\n",
    "\n",
    "        R = np.linalg.cholesky(B_)\n",
    "        q = np.linalg.inv(R.T) @ p\n",
    "\n",
    "        lam_new = lam + (np.linalg.norm(p) / np.linalg.norm(q)) ** 2 * (\n",
    "            (np.linalg.norm(p) - delta) / delta\n",
    "        )\n",
    "        if lam_new == lam or (count >= max_iter and np.linalg.norm(p) <= delta):\n",
    "            # return p\n",
    "            break\n",
    "        else:\n",
    "            lam = lam_new\n",
    "    return p\n",
    "\n",
    "def hard_case(gradient, hessian, delta, lambda_j, vector, min_index):\n",
    "    temp = gradient.T @ hessian @ gradient\n",
    "    tau = min(1, np.linalg.norm(gradient) ** 3 / (delta * temp))\n",
    "    z = vector[:, min_index]  # eigenvector of B corresponding to lamda_1\n",
    "    lamda_1 = lambda_j[min_index]\n",
    "    p = 0\n",
    "    for i in range(np.shape(lambda_j)[0]):\n",
    "        if lamda_1 != lambda_j[i]:\n",
    "            p += -((vector[:, i].T @ gradient) / (lambda_j[i] - lamda_1) * vector[:, i])\n",
    "    p += tau * z\n",
    "    return p\n",
    "\n",
    "def cauchy_point(gradient, hessian, delta):\n",
    "    temp = gradient.T @ hessian @ gradient\n",
    "    if temp <= 0:\n",
    "        tau = 1\n",
    "    else:\n",
    "        tau = min(1, np.linalg.norm(gradient) ** 3 / (delta * temp))\n",
    "    return -tau * delta / np.linalg.norm(gradient) * gradient\n",
    "\n",
    "def trust_region(_x, fun, fun_g, fun_h, _delta, delta_max, eta=0.2, max_iter=1000):\n",
    "    xs = [_x]\n",
    "    deltas = [_delta]\n",
    "    x = _x\n",
    "    delta = _delta\n",
    "    count = 0\n",
    "    while count < max_iter:\n",
    "        count += 1\n",
    "        g = fun_g(x)\n",
    "        B = fun_h(x)\n",
    "        if is_pos_def(B):\n",
    "            if np.linalg.norm(np.linalg.inv(B) @ g) <= delta:\n",
    "                p = -np.linalg.inv(B) @ g\n",
    "            else:\n",
    "                p = trust_region_subproblem(g, B, 0, delta)\n",
    "        else:\n",
    "            p = trust_region_subproblem(g, B, -min(np.linalg.eigvals(B))+1e-5, delta)  # set the first lambda to -lambda_1\n",
    "\n",
    "        rho = (fun(x) - fun(x + p)) / (- g.T @ p - 0.5 * p.T @ B @ p)  # should we use (4.1)? t?\n",
    "\n",
    "        if rho < 1 / 4:\n",
    "            delta = delta/4\n",
    "        else:\n",
    "            if rho > 3 / 4 and np.linalg.norm(p) == delta:\n",
    "                delta = min(2 * delta, delta_max)\n",
    "\n",
    "        if rho > eta:\n",
    "            x = x + p\n",
    "        else:\n",
    "            break  # x_k = x_k+1\n",
    "\n",
    "        xs.append(x)\n",
    "        deltas.append(delta)\n",
    "\n",
    "    return np.array(xs), fun(x), count, np.array(deltas)\n",
    "\n",
    "def cal_rate_convergence(xs, fun):\n",
    "    x_ = xs[-1]\n",
    "    x1 = xs[:-1]\n",
    "    x2 = xs[1:]\n",
    "    res = []\n",
    "    for i in range(len(x1)):\n",
    "        if fun(x1[i])-fun(x_) == 0:\n",
    "            continue\n",
    "        res.append((fun(x2[i])-fun(x_))/(fun(x1[i])-fun(x_)))\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_x(step,start,end,dimension): #set the start&end coord, step size and dimension number pf input set\n",
    "    xi=np.arange(start, end, step); x=xi #initialization of input xi for each dimension\n",
    "    for i in range(dimension-1):\n",
    "        x=np.vstack((np.around(x,decimals=9),np.around(xi,decimals=9))) #make x to d dimensions, from xi\n",
    "    return x\n",
    "\n",
    "def f1(x):\n",
    "    alpha=1000 #set alpha\n",
    "    dim=x.shape[0] #dimension number\n",
    "    y=0\n",
    "    for i in range(dim):\n",
    "        y+=alpha**(i/(dim-1))*x[i]**2\n",
    "    return y\n",
    "\n",
    "def gradf1(x):\n",
    "    alpha=1000 #set alpha\n",
    "    dim=x.shape[0] #dimension number\n",
    "    #dtsize=x.shape[1] #data point number\n",
    "    result=np.zeros(dim)\n",
    "    #print(dim,dtsize)\n",
    "    for i in range(dim):\n",
    "        result[i]=2*(alpha**(i/(dim-1)))*x[i]\n",
    "    return result\n",
    "\n",
    "def hessf1(x):\n",
    "    alpha=1000 #set alpha\n",
    "    dim=x.shape[0] #dimension number\n",
    "    #dtsize=x.shape[1] #data point number\n",
    "    result=np.zeros((dim,dim))\n",
    "    for i in range(dim):\n",
    "        result[i,i]=2*(alpha**(i/(dim-1)))\n",
    "    return result\n",
    "\n",
    "f2 = lambda x: (1-x[0])**2+100*(x[1]-x[0]**2)**2; # rosenbrok v2\n",
    "\n",
    "def gradf2(x):\n",
    "    g1 = -400*x[0]*(x[1]-x[0]**2)-2*(1-x[0])\n",
    "    g2 = 200*(x[1]-x[0]**2)\n",
    "    return np.array([g1, g2])\n",
    "\n",
    "def hessf2(x):\n",
    "    h11 = 2+1200*x[0]**2-400*x[1]\n",
    "    h1221 = -400*x[0]\n",
    "    h22 = 200\n",
    "    return np.array([[h11,h1221],[h1221,h22]])\n",
    "\n",
    "def plot2d(x0,f,f_input,fcount,f_y,name):\n",
    "    x=init_x(0.01,-3.5,3.5,2)\n",
    "    X1,X2 = np.meshgrid(x[0], x[1]) #generate all the data point\n",
    "    dtsize=X1.shape[0] #data point number\n",
    "    Y=np.zeros((dtsize,dtsize)) #initialize output results to 2D\n",
    "    for i in range(dtsize):\n",
    "        for j in range(dtsize):\n",
    "            X=np.vstack((np.around(X1[i,j],decimals=9),np.around(X2[i,j],decimals=9))) #choose every combination of 2D inputs\n",
    "            Y[i,j]=f(X) #store the results\n",
    "    fx1=np.zeros(fcount+1)\n",
    "    fx2=np.zeros(fcount+1)\n",
    "    for i in range(fcount+1):\n",
    "        if(i<=0):\n",
    "            fx1[i]=x0[0]\n",
    "            fx2[i]=x0[1]\n",
    "        else:\n",
    "            fx1[i]=f_input[i-1][0]\n",
    "            fx2[i]=f_input[i-1][1]\n",
    "    #plot in 2D with color\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlabel('X1')\n",
    "    ax.set_ylabel('X2')\n",
    "    lv=[0,1,3,10,30,100,500,2000,4000,7000,11000,14000,17000,25000]\n",
    "    Cset = plt.contourf(X1, X2, Y, levels=lv,norm=colors.PowerNorm(gamma=0.25),cmap='coolwarm_r')\n",
    "    #lv=[0,1,3,6,10,15,25,50,100,150,200,250,300,350,410,480,560,650,750]\n",
    "    #Cset = plt.contourf(X1, X2, Y, levels=lv,norm=colors.PowerNorm(gamma=0.5),cmap='coolwarm_r')\n",
    "    #Cset = plt.contourf(X1, X2, Y, levels=20,cmap='coolwarm_r')\n",
    "    plt.plot(fx1, fx2,c=\"k\")\n",
    "    plt.colorbar(Cset)\n",
    "    plt.title(name)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(name+'_2d.pdf')\n",
    "\n",
    "#x0 = np.array([-1.8, 2.1])\n",
    "\n",
    "#f1_input, f1_y, f1_count, f1_dels = trust_region(np.array(x0), f1, gradf1, hessf1, 0.1, 5)\n",
    "#plot2d(x0,f1,f1_input,f1_count,f1_y,name=\"f1 start at (%.1f,%.1f)\" %(x0[0],x0[1]))\n",
    "#f2_input, f2_y, f2_count, f2_dels = trust_region(np.array(x0), f2, gradf2, hessf2, 0.1, 5)\n",
    "#plot2d(x0,f2,f2_input,f2_count,f2_y,name=\"f2 start at (%.1f,%.1f) when Δ0=0.5\" %(x0[0],x0[1]))\n",
    "#f3_input, f3_y, f3_count, f3_dels = trust_region(np.array(x0), func.log_ellipsoid, func.log_ellipsoid_d1, func.log_ellipsoid_d2, 0.1, 5)\n",
    "#plot2d(x0,func.log_ellipsoid,f3_input,f3_count,f3_y,name=\"f3 start at (%.1f,%.1f)\" %(x0[0],x0[1]))\n",
    "#f4_input, f4_y, f4_count, f4_dels = trust_region(np.array(x0), func.f_4, func.f_4grad, func.f_4hessian, 0.1, 5)\n",
    "#plot2d(x0,func.f_4,f4_input,f4_count,f4_y,name=\"f4 start at (%.1f,%.1f)\" %(x0[0],x0[1]))\n",
    "#f5_input, f5_y, f5_count, f5_dels = trust_region(np.array(x0), func.f_5, func.f_5grad, func.f_5hessian, 0.1, 5)\n",
    "#plot2d(x0,func.f_5,f5_input,f5_count,f5_y,name=\"f5 start at (%.1f,%.1f)\" %(x0[0],x0[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nimport handin1 as func\\nf3_input, f3_y, f3_count, f3_dels = trust_region(np.array(x0), func.log_ellipsoid, func.log_ellipsoid_d1, func.log_ellipsoid_d2, 0.1, 5)\\nplot2d(x0,func.log_ellipsoid,f3_input,f3_count,f3_y,name=\"f3 start at (%.1f,%.1f)\" %(x0[0],x0[1]))\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "'''\n",
    "import handin1 as func\n",
    "f3_input, f3_y, f3_count, f3_dels = trust_region(np.array(x0), func.log_ellipsoid, func.log_ellipsoid_d1, func.log_ellipsoid_d2, 0.1, 5)\n",
    "plot2d(x0,func.log_ellipsoid,f3_input,f3_count,f3_y,name=\"f3 start at (%.1f,%.1f)\" %(x0[0],x0[1]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_convergence(xs, fun, label=\"\", color='b', name=\"\"):\n",
    "    # _x = xs[-1]\n",
    "    # xx = cal_convergence(xs)\n",
    "    xx = cal_rate_convergence(xs, fun)\n",
    "    it = np.arange(xx.shape[0]-1)\n",
    "    plt.plot(it, xx[:-1], c=color, label=label)\n",
    "    # plt.legend()\n",
    "    plt.yscale(\"log\")\n",
    "    plt.title(name)\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.ylabel(\"convergence rate of x\")\n",
    "\n",
    "def plot_deltas(dels, label=\"\", name=\"\"):\n",
    "    ax = plt.figure().gca()\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    it = np.arange(dels.shape[0])\n",
    "    #itint = range(min(it), math.ceil(max(it))+1)\n",
    "    plt.plot(it, dels, label=label)\n",
    "    #plt.xticks(itint)\n",
    "    plt.title(name)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Δk\")\n",
    "\n",
    "def test(count,dels,func_name):\n",
    "    # plot_convergence(xs, func.ellipsoid, name=\"convergence rate of f4\")\n",
    "    plot_deltas(dels, name=func_name)\n",
    "    plt.savefig(func_name+\".pdf\")\n",
    "    plt.show()\n",
    "    print(count)\n",
    "\n",
    "#f_count=np.vstack((f1_count,np.vstack((f2_count,np.vstack((f3_count,np.vstack((f4_count,f5_count))))))))\n",
    "#print(f_count,\"\\n\",f_count[:,0])\n",
    "#f_dels=np.vstack((f1_dels,np.vstack((f2_dels,np.vstack((f3_dels,np.vstack((f4_dels,f5_dels))))))))\n",
    "\n",
    "#test(f1_count, f1_dels,\"f1 Δk changes\")\n",
    "#test(f2_count, f2_dels,\"f2 Δk changes\")\n",
    "#test(f3_count, f3_dels,\"f3 Δk changes\")\n",
    "#test(f4_count, f4_dels,\"f4 Δk changes\")\n",
    "#test(f5_count, f5_dels,\"f5 Δk changes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-1-a1647f77ec45>:79: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  rho = (fun(x) - fun(x + p)) / (- g.T @ p - 0.5 * p.T @ B @ p)  # should we use (4.1)? t?\n",
      "Trust-Region & 1.34 & 0.69625\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def opt(f,g,h):\n",
    "    # time\n",
    "    t=0\n",
    "    # number of iteration\n",
    "    i=0\n",
    "    s=0\n",
    "    points = 50\n",
    "    for i in range(points):\n",
    "        dim = 2 #point dimension number\n",
    "        x = np.random.rand(dim)*5 #generate random point\n",
    "        tmp1 = time.process_time()\n",
    "        f_input, f_y, f_count, f_dels = trust_region(np.array(x), f, g, h, 0.1, 5)\n",
    "        tmp2 = time.process_time()\n",
    "        runtime=tmp2-tmp1\n",
    "        t += runtime\n",
    "        i += f_count\n",
    "        #if np.linalg.norm(g(f_input))<=1e-6:\n",
    "        #    s+=1\n",
    "    print(\"Trust-Region &\",i/points,\"&\",t/points)\n",
    "\n",
    "#opt(f1, gradf1, hessf1)\n",
    "opt(f2, gradf2, hessf2)\n",
    "#opt(func.log_ellipsoid, func.log_ellipsoid_d1, func.log_ellipsoid_d2)\n",
    "#opt(func.f_4, func.f_4grad, func.f_4hessian)\n",
    "#opt(func.f_5, func.f_5grad, func.f_5hessian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_k(gradient, hessian, method):\n",
    "    if(method == 'Newton'):\n",
    "        return -np.linalg.inv(hessian) @ gradient #Newton's method\n",
    "    elif(method == 'Steepest'):\n",
    "        return -gradient #Steepest descent\n",
    "    else:\n",
    "        raise Exception('Invalid method')\n",
    "\n",
    "def backtracking(p, x, old_x,old_alpha, old_slope, function, gradient, method): # 3.1 on page 37 and page 59\n",
    "    alpha = 1\n",
    "    '''\n",
    "    if(method == \"Steepest\" and old_x is not None):\n",
    "        alpha=2*(function(x)-function(old_x))/(gradient(x+0*p).T @ p)\n",
    "        alpha=min(1,1.01*alpha)\n",
    "        if old_slope != None:\n",
    "            alpha = old_alpha * old_slope\n",
    "        else:\n",
    "            alpha = old_alpha\n",
    "    print(alpha)\n",
    "    '''\n",
    "    rho = 0.8 # Good values in previous courses\n",
    "    c   = 0.2 # Good values in previous courses\n",
    "    while function(x + alpha * p) > function(x) + c * alpha * gradient(x).T @ p:\n",
    "        alpha = rho * alpha\n",
    "    return alpha\n",
    "\n",
    "def is_converged(tolerance, x=None, oldx=None, f=None, f_grad=None, p=None, check_method='relative_x_dist'):\n",
    "    if check_method == 'gradient_norm':\n",
    "        return np.linalg.norm(f_grad(x)) < tolerance\n",
    "    elif check_method == 'relative_x_dist':\n",
    "        return oldx is not None and np.abs(f(x) - f(oldx)) < tolerance #or np.linalg.norm(x - oldx)/np.linalg.norm(oldx)\n",
    "    elif check_method == 'pk':\n",
    "        return np.all(np.abs(p) < tolerance)\n",
    "\n",
    "#is_pos_def = lambda x: np.all(np.linalg.eigvals(x) > 0); #check if a matrix is positive definite or not\n",
    "def is_pos_def(x):\n",
    "    return np.all(np.linalg.eigvals(x) > 0)\n",
    "\n",
    "def linesearch(start_point, function, gradient, hessian=None, method='Steepest', max_iter=50, tolerance=1e-03, name=\"\"): #default configuration\n",
    "    x = start_point\n",
    "    old_x = None\n",
    "    old_alpha = None\n",
    "    old_slope = None\n",
    "    old_p = None\n",
    "    success = False\n",
    "    idx = 0\n",
    "    xs=[]; xg=[]; xf=[]; xfdivi=[]\n",
    "    for idx in range(1, max_iter+1):\n",
    "        hess=hessian(x)\n",
    "        if(method == 'Newton'):\n",
    "            if not is_pos_def(hess):\n",
    "                min_diag = np.amin(hess.diagonal()) #find the minimum diagonal element of hessian(x)\n",
    "                tol = 0\n",
    "                if min_diag <= 0: tol = -min_diag + tolerance #change the tolerance\n",
    "                d=hess.shape[0]\n",
    "                hess_changed = hess + tol*np.identity(d) #adding tol times an identity matrix of size of hessian(x)\n",
    "                while not is_pos_def(hess_changed):\n",
    "                    tol = max(2*tol, tolerance) #decide the tol while iterating\n",
    "                    hess_changed = hess + tol * np.identity(d)\n",
    "            else:\n",
    "                hess_changed = hess\n",
    "        else:\n",
    "            hess_changed = hess\n",
    "        p     = get_p_k(gradient(x), hess_changed, method) # Getting direction vector\n",
    "        alpha = backtracking(p, x, old_x, old_alpha, old_slope, function, gradient, method) #or use   # Getting step length\n",
    "        x     = x + alpha * p # Calculating new x\n",
    "        #xx = list(x)\n",
    "        if(old_x is not None):\n",
    "            xfdivi.append(function(x)/function(old_x))\n",
    "        xf.append(function(x))\n",
    "        xg.append(gradient(x))\n",
    "        xs.append(x)\n",
    "        if is_converged(tolerance, x, old_x, function, gradient, p, 'gradient_norm'):\n",
    "            success = True\n",
    "            break\n",
    "        if(method == 'Steepest'): # Saving values for steepest decent method\n",
    "            old_alpha = alpha\n",
    "            if idx > 1:\n",
    "                old_slope = (gradient(old_x).T @ old_p)/(gradient(x).T @ p)\n",
    "            old_x     = x\n",
    "            old_p     = p\n",
    "    res = {\n",
    "        'iterations': idx,\n",
    "        'final_x': x,\n",
    "        'success': success,\n",
    "    }\n",
    "    #plot_function(np.array(xfdivi), name+\" \"+method)\n",
    "    #plot_gradient(np.array(xg), name+\" \"+method)\n",
    "    #plot_convergence(np.array(xs), name+\" \"+method)\n",
    "    #plot_distance(np.array(xs), name+\" \"+method)\n",
    "    return res,xs,xf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Steepest & 240.98 & 22.315\n"
     ]
    }
   ],
   "source": [
    "def opt_line(f,g,h):\n",
    "    # time\n",
    "    t=0\n",
    "    # number of iteration\n",
    "    i=0\n",
    "    points = 50\n",
    "    for i in range(points):\n",
    "        dim = 3 #point dimension number\n",
    "        x = np.random.rand(dim)*5 #generate random point\n",
    "        tmp1 = time.process_time()\n",
    "        res,xs,xf = linesearch(x, f, g, h, method='Steepest', max_iter=12000, tolerance=1e-6, name=\"\")\n",
    "        tmp2 = time.process_time()\n",
    "        runtime=tmp2-tmp1\n",
    "        t += runtime\n",
    "        i += res['iterations']\n",
    "    print(\"Steepest &\",i/points,\"&\",t/points)\n",
    "\n",
    "#opt_line(f1, gradf1, hessf1)\n",
    "#opt_line(f2, gradf2, hessf2)\n",
    "opt_line(func.log_ellipsoid, func.log_ellipsoid_d1, func.log_ellipsoid_d2)\n",
    "#opt_line(func.f_4, func.f_4grad, func.f_4hessian)\n",
    "#opt_line(func.f_5, func.f_5grad, func.f_5hessian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nopt_line(func.f_4, func.f_4grad, func.f_4hessian)\\nopt_line(func.f_5, func.f_5grad, func.f_5hessian)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "def opt_line(f,g,h):\n",
    "    # time\n",
    "    t=0\n",
    "    # number of iteration\n",
    "    i=0\n",
    "    points = 500\n",
    "    for i in range(points):\n",
    "        dim = 2 #point dimension number\n",
    "        x = np.random.rand(dim)*5 #generate random point\n",
    "        tmp1 = time.process_time()\n",
    "        res,xs,xf = linesearch(x, f, g, h, method='Newton', max_iter=50, tolerance=1e-6, name=\"\")\n",
    "        tmp2 = time.process_time()\n",
    "        runtime=tmp2-tmp1\n",
    "        t += runtime\n",
    "        i += res['iterations']\n",
    "    print(\"Newton &\",i/points,\"&\",t/points)\n",
    "\n",
    "#opt_line(f1, gradf1, hessf1)\n",
    "#opt_line(f2, gradf2, hessf2)\n",
    "#opt_line(func.log_ellipsoid, func.log_ellipsoid_d1, func.log_ellipsoid_d2)\n",
    "'''\n",
    "opt_line(func.f_4, func.f_4grad, func.f_4hessian)\n",
    "opt_line(func.f_5, func.f_5grad, func.f_5hessian)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}